{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Function Calling with the Vertex AI Gemini API & Python SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHPv2myT2cx"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases.\n",
    "\n",
    "### Calling functions from Gemini\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "Function calling is similar to [Vertex AI Extensions](https://cloud.google.com/vertex-ai/docs/generative-ai/extensions/overview) in that they both generate information about functions. The difference between them is that function calling returns JSON data with the name of a function and the arguments to use in your code, whereas Vertex AI Extensions returns the function and calls it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrkcqHrrwMAo"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use the Vertex AI Gemini API with the Vertex AI SDK for Python to make function calls via the Gemini Pro (`gemini-pro`) model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Install the Vertex AI SDK for Python\n",
    "- Use the Vertex AI Gemini API to interact with the Gemini Pro (`gemini-pro`) model:\n",
    "    - Generate function calls from a text prompt to get the weather for a given location\n",
    "    - Generate function calls from a text prompt and call an external API to geocode addresses\n",
    "    - Generate function calls from a chat prompt to help retail users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9nEPojogw-g"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11Gu7qNgx1p"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7UyNVSiyQ96"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. Run the following cell to restart the current kernel.\n",
    "\n",
    "The restart process might take a minute or so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the restart is complete, continue to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXQZrM5hQeKb"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Wait for the kernel to finish restarting before you continue. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=7JmLlsnMxynpa6bF3fK8LlZ54XdtxV&access_type=offline&code_challenge=fdsNvK0NRcWrIDWrCi97X6PKa1eok-poezZUrv6ykno&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/jamesharding/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"sungyong-internship\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()\n",
    "else:\n",
    "    !gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Define Google Cloud project information (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, specify the Google Cloud project information to use. In the following cell, you specify your project information, import the Vertex AI package, and initialize the package. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    # Define project information\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "    LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "    # Initialize Vertex AI\n",
    "    import vertexai\n",
    "\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lslYAvw37JGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    FunctionDeclaration,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Examples\n",
    "\n",
    "### Why function calling?\n",
    "\n",
    "When working with a generative text model, it can be difficult to coerce the LLM to give consistent responses in a structured format such as JSON. Function calling makes it easy to work with LLMs via prompts and unstructured inputs, and have the LLM return a structured response that can be used to call an external function.\n",
    "\n",
    "You can think of function calling as a way to get structured output from user prompts and function definitions, use that structured output to make an API request to an external system, then return the function response to the LLM to generate a response to the user. In other words, function calling in Gemini extracts structured parameters from unstructured text or messages from users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "### Use the Gemini Pro model\n",
    "\n",
    "The Gemini Pro (`gemini-pro`) model is designed to handle natural language tasks, multiturn text and code chat, and code generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2998506fe6d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple function calling example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, you'll use function calling to set up a weather API request for users to obtain the current conditions in a given location. Function parameters are specified as a Python dictionary in accordance with the [OpenAPI JSON schema format](https://spec.openapis.org/oas/v3.0.3#schemawr).\n",
    "\n",
    "Consider an example weather API function that takes in an argument for the user's location, as in:\n",
    "\n",
    "```python\n",
    "def get_current_weather(location):\n",
    "    ...\n",
    "```\n",
    "\n",
    "You'll start by specifying a function declaration and parameters needed to make a request our example weather API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_current_weather_func = FunctionDeclaration(\n",
    "    name=\"get_current_weather\",\n",
    "    description=\"Get the current weather in a given location\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Location\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then define a tool for the LLM to call that includes the `get_current_weather_func`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_tool = Tool(\n",
    "    function_declarations=[get_current_weather_func],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then instruct the model to generate content, include the `tool` that you just created, to generate a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      function_call {\n",
       "        name: \"get_current_weather\"\n",
       "        args {\n",
       "          fields {\n",
       "            key: \"location\"\n",
       "            value {\n",
       "              string_value: \"Boston\"\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  finish_reason: STOP\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HARASSMENT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HATE_SPEECH\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 8\n",
       "  total_token_count: 8\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the weather like in Boston?\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\"temperature\": 0},\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the function call portion of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"get_current_weather\"\n",
       "args {\n",
       "  fields {\n",
       "    key: \"location\"\n",
       "    value {\n",
       "      string_value: \"Boston\"\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response includes a function signature that can be used to call the weather API. At this point, you have everything that you need to form a request body and make an API call to an external system. Nice work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex function calling example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you'll generate a function call that has a more complex structure. You'll use the function response to make an API call that converts an address to latitude and longitude coordinates.\n",
    "\n",
    "Start by defining a function declaration within a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_location = FunctionDeclaration(\n",
    "    name=\"get_location\",\n",
    "    description=\"Get latitude and longitude for a given location\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"poi\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Point of interest\"\n",
    "        },\n",
    "        \"street\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Street name\"\n",
    "        },\n",
    "        \"city\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"City name\"\n",
    "        },\n",
    "        \"county\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"County name\"\n",
    "        },\n",
    "        \"state\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"State name\"\n",
    "        },\n",
    "        \"country\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Country name\"\n",
    "        },\n",
    "        \"postal_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Postal code\"\n",
    "        },\n",
    "    },\n",
    "},\n",
    ")\n",
    "\n",
    "location_tool = Tool(\n",
    "    function_declarations=[get_location],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can call the model to generate a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"get_location\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"street\"\n",
       "      value {\n",
       "        string_value: \"2036 Sterling Trace Dr\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"state\"\n",
       "      value {\n",
       "        string_value: \"TX\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"country\"\n",
       "      value {\n",
       "        string_value: \"US\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"city\"\n",
       "      value {\n",
       "        string_value: \"Keller\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I want to get the lat/lon coordinates for the following address:\n",
    "1600 Amphitheatre Pkwy, Mountain View, CA 94043, US\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "I want to get the lat/lon coordinates for the following address:\n",
    "2036 Sterling Trace Dr, Keller, TX 76248, US\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\"temperature\": 0},\n",
    "    tools=[location_tool],\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now extract the results from the function response and make an API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state TX\n",
      "city Keller\n",
      "street 2036 Sterling Trace Dr\n",
      "country US\n",
      "https://nominatim.openstreetmap.org/search?state=\"TX\"&city=\"Keller\"&street=\"2036 Sterling Trace Dr\"&country=\"US\"&format=json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'place_id': 362211439,\n",
       "  'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. http://osm.org/copyright',\n",
       "  'osm_type': 'way',\n",
       "  'osm_id': 232901552,\n",
       "  'lat': '32.969878052663226',\n",
       "  'lon': '-97.23902584170965',\n",
       "  'class': 'place',\n",
       "  'type': 'house',\n",
       "  'place_rank': 30,\n",
       "  'importance': 9.999999994736442e-08,\n",
       "  'addresstype': 'place',\n",
       "  'name': '',\n",
       "  'display_name': '2036, Sterling Trace Drive, Keller, Tarrant County, Texas, 76248, United States',\n",
       "  'boundingbox': ['32.9698281', '32.9699281', '-97.2390758', '-97.2389758']}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose = True\n",
    "\n",
    "x = response.candidates[0].content.parts[0].function_call.args\n",
    "if verbose:\n",
    "    for i in x:\n",
    "        print(i, x[i])\n",
    " \n",
    "\n",
    "url = \"https://nominatim.openstreetmap.org/search?\"\n",
    "\n",
    "for i in x:\n",
    "    url += '{}=\"{}\"&'.format(i, x[i])\n",
    "url += \"format=json\"\n",
    "\n",
    "if verbose:\n",
    "    print(url)  \n",
    "\n",
    "x = requests.get(url)\n",
    "\n",
    "content = x.json()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'proto.marshal.collections.maps.MapComposite'>\n",
      "street 1600 Amphitheatre Pkwy\n",
      "state CA\n",
      "city Mountain View\n",
      "postal_code 94043\n",
      "country US\n"
     ]
    }
   ],
   "source": [
    "x = response.candidates[0].content.parts[0].function_call.args\n",
    "print(type(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! You were able to construct a function and tool that the LLM used to output the parameters necessary for a function call, then you actually made the function call to obtain the coordinates of the specified location.\n",
    "\n",
    "Here we used the [OpenStreetMap Nominatim API](https://nominatim.openstreetmap.org/ui/search.html) to geocode an address to make it easy to use and learn in this notebook. If you're working with large amounts of maps or geolocation data, you can use the [Google Maps Geocoding API](https://developers.google.com/maps/documentation/geocoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling in a chat session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you'll use the chat model in Gemini to help customers get information about products in a store.\n",
    "\n",
    "You'll start by defining multiple functions within a tool for getting product information, getting the location of stores, and placing an order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_product_info_func = FunctionDeclaration(\n",
    "    name=\"get_product_sku\",\n",
    "    description=\"Get the SKU for a product\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"product_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Product name\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    ")\n",
    "\n",
    "get_store_location_func = FunctionDeclaration(\n",
    "    name=\"get_store_location\",\n",
    "    description=\"Get the location of the closest store\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Location\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    ")\n",
    "\n",
    "place_order_func = FunctionDeclaration(\n",
    "    name=\"place_order\",\n",
    "    description=\"Place an order\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"product\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Product name\"\n",
    "        },\n",
    "        \"account\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Account number\"\n",
    "        },\n",
    "        \"address\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Shipping address\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    ")\n",
    "\n",
    "retail_tool = Tool(\n",
    "    function_declarations=[get_product_info_func, \n",
    "                           get_store_location_func, \n",
    "                           place_order_func,\n",
    "                          ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also use function calling in a multi-turn chat session, and you can specify tools when creating a model to avoid having to send them with every request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\", \n",
    "                        generation_config={\"temperature\": 0},\n",
    "                        tools=[retail_tool])\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's start the conversation by asking if they have a certain product in stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"get_product_sku\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"product_name\"\n",
       "      value {\n",
       "        string_value: \"Pixel 8 Pro\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Do you have the Pixel 8 Pro in stock?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the response includes a structured function call that we can use to communicate with external systems.\n",
    "\n",
    "In reality, you would execute function calls against an external system or database. Since this notebook focuses on the ability to extract function parameter and generate function calls, you'll use mock data to feed responses back to the model rather that using an actual API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is where you would make an API request to return the status of their order.\n",
    "# Use synthetic data to simulate a response payload from an external API.\n",
    "\n",
    "api_response = {\"sku\": \"GA04834-US\", \"in_stock\": \"yes\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's include details from the external API call and generate a response to the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: \" Yes, we have the Pixel 8 Pro in stock.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"get_product_sku\",\n",
    "        response={\n",
    "            \"content\": api_response,\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the user might ask where they can buy a phone from a nearby store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"get_store_location\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"location\"\n",
       "      value {\n",
       "        string_value: \"Mountain View, CA\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Where can I buy one near Mountain View, CA?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a response with another structured function call, this time it's set up to use a different function from our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is where you would make an API request to get the location of the store closest to the user.\n",
    "# Use synthetic data to simulate a response payload from an external API.\n",
    "\n",
    "api_response = {\"store\": \"1600 Amphitheatre Pkwy, Mountain View, CA 94043, US\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's include details from the external API call and generate a response to the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: \" There is a store at 1600 Amphitheatre Pkwy, Mountain View, CA 94043, US.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"get_store_location\",\n",
    "        response={\n",
    "            \"content\":  api_response,\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the user might ask to order a phone and have it shipped to an address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"place_order\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"product\"\n",
       "      value {\n",
       "        string_value: \"Pixel 8 Pro\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"address\"\n",
       "      value {\n",
       "        string_value: \"1155 Borregas Ave, Sunnyvale, CA 94089\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"account\"\n",
       "      value {\n",
       "        string_value: \"1234567890\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I'd like to order a Pixel 8 Pro and have it shipped to 1155 Borregas Ave, Sunnyvale, CA 94089.\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We extracted the user's desired product, address, fetched their account number, and now we can call an API to place the order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is where you would make an API request to return the status of their order.\n",
    "# Use synthetic data to simulate a response payload from an external API.\n",
    "\n",
    "api_response = {\"payment_status\": \"paid\", \"order_number\": 12345, \"est_arrival\": \"2 days\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's include details from the external API call and generate a response to the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: \" OK. Your order has been placed and will arrive in 2 days. Your order number is 12345.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"place_order\",\n",
    "        response={\n",
    "            \"content\":  api_response,\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're done! You successfully had a multi-turn conversation with Gemini, generated function calls, handled passing (mock) data back to the model, and generated messages that made use of the function responses."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
