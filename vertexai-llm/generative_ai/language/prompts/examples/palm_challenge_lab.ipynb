{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# PaLM Challenge Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "You've just spent some time running through many of the elements related to Google's PaLM API, its use and its API. This challenge lab is designed to test your knoledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Objectives\n",
    "\n",
    "- Load the Vertex AI language models\n",
    "- Authenticate your environment\n",
    "- Demonstrate the use of prompt design, ideation, text classification, and text extraction\n",
    "\n",
    "Most of the following Python notebook cells have missing or incomplete code sections. Your challenge is to complete each cell, run it to test for correctness, and then move on. When all the cells are working, you have completed the challenge.\n",
    "\n",
    "**Note: The notebooks used in the PaLM labs may all be viewed directly on Github [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete the following pip command\n",
    "#! pip3 install --upgrade --user google-cloud-aiplatform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart your notebook Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* Use the instructions found [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env) in the block below, insert the lines required to set the project ID and location (us-central1 works for sure) variables. Then import and initialize the vertexai library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GCP Project Information\n",
    "\n",
    "import yaml\n",
    "\n",
    "config_file = '/Users/jamesharding/repo/ml-gcp/config/p1.yaml'\n",
    "\n",
    "def get_variables(config_file):\n",
    "  with open(config_file, 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "  return data\n",
    "\n",
    "var = get_variables(config_file)\n",
    "\n",
    "for key,val in var.items():\n",
    "  exec(key + '=val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify parameters are set\n",
    "if not (PROJECT_ID and REGION and GCS_BUCKET):\n",
    "    from absl import logging\n",
    "    logging.error('Please set all required parameters.')\n",
    "else:\n",
    "    print('Parameters set:')\n",
    "    print('\\t\\tROJECT_ID:\\t {}'.format(PROJECT_ID))\n",
    "    print('\\t\\tPROJECT_NUMBER:\\t {}'.format(PROJECT_NUMBER))\n",
    "    print('\\t\\tREGION:\\t\\t {}'.format(REGION))\n",
    "    print('\\t\\tGCS_BUCKET:\\t {}'.format(GCS_BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6"
   },
   "outputs": [],
   "source": [
    "# insert the requisite steps here\n",
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()\n",
    "else:\n",
    "    ! gcloud auth application-default login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Import libraries\n",
    "Import the following language model classes:\n",
    "- ChatModel\n",
    "- InputOutputTextPair\n",
    "- TextEmbeddingModel\n",
    "- TextGenerationModel\n",
    "\n",
    "from the Vertex AI, preview, language models module. \n",
    "\n",
    "For display purposes, also import Markdown and display from IPython display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini Model\n",
    "from vertexai.preview.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4zjV4alsiVql"
   },
   "outputs": [],
   "source": [
    "# Complete the two imports\n",
    "# PaLM Language Model\n",
    "from vertexai.language_models import ChatModel, InputOutputTextPair, TextEmbeddingModel, TextGenerationModel\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Google's PaLM technology to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "#### Load the TextGenerationModel, and store it in a the `generation_model` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2998506fe6d1"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEAJ0ipmbndQ"
   },
   "source": [
    "### Prompt design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCgBDJvNRCF5"
   },
   "source": [
    "Create a prediction around the prompt, \"Tell me about Google's PaLM API.\" Set the `temperature` for the least open ended response ans set the `max_output_tokens` for the longest response possible with the text-bison@001 model. Leave the top_k and top_p with their defaults. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cx_o455SRCF5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The temperature for this response is set to 0.5, which means that it is less open ended \n",
    "than a response with a higher temperature. This is because the response focuses on providing \n",
    "factual information about the PaLM API, rather than generating creative text formats, like \n",
    "poems, code, scripts, musical pieces, email, letters, etc.\n",
    "\"\"\"\n",
    "\n",
    "temperature = 0.5\n",
    "\n",
    "# The largest max_output_tokens for the PaLM API is 1024 tokens\n",
    "max_output_tokens = 1024\n",
    "\n",
    "parameters = {\n",
    "        \"temperature\": temperature,  # Temperature controls the degree of randomness in token selection.\n",
    "        \"max_output_tokens\": max_output_tokens,  # Token limit determines the maximum amount of text output.\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google's Pathways Language Model (PaLM) is a large language model that was trained by a team of engineers and scientists at Google AI. The model was trained on a massive dataset of text and code, and it is capable of generating text, translating languages, answering questions, and writing different kinds of creative content.\n",
      "\n",
      "The PaLM API is a tool that allows developers to access the power of PaLM in their own applications. The API provides a simple and easy-to-use interface that allows developers to call PaLM's methods and get results back in a matter of seconds.\n",
      "\n",
      "The PaLM API is currently in beta, but it is already being used by developers to build a variety of applications, including chatbots, question-answering systems, and creative writing tools.\n",
      "\n",
      "Here are some of the things that PaLM can do:\n",
      "\n",
      "* Generate text: PaLM can generate text of different lengths, styles, and genres. It can write different kinds of creative content, such as poems, code, scripts, and musical pieces.\n",
      "* Translate languages: PaLM can translate languages from one language to another. It can also translate between different dialects of the same language.\n",
      "* Answer questions: PaLM can answer a variety of questions, both factual and open-ended. It can also provide explanations for its answers.\n",
      "* Write different kinds of creative content: PaLM can write different kinds of creative content, such as poems, code, scripts, and musical pieces.\n",
      "\n",
      "The PaLM API is a powerful tool that can be used to build a variety of applications. It is still in beta, but it is already showing great promise.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about Google's PaLM API.\"\n",
    "\n",
    "response = generation_model.predict(prompt, **parameters)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsglQtgDRCF5"
   },
   "source": [
    "### Ideation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BP1BKWiRCF6"
   },
   "source": [
    "Use the below template to get your model to give you 5 title ideas for a training course on Google's Generative AI technologies. Use display and Markdown to render the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2USfPyOuFhlB"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Introduction to Generative AI\n",
       "* Generative AI for Beginners\n",
       "* Advanced Generative AI\n",
       "* Generative AI for Data Scientists\n",
       "* Generative AI for Creative Professionals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Give me 5 title ideas for a training course o Google's Generative AI technologies.\"\n",
    "\n",
    "response = generation_model.predict(prompt, **parameters)\n",
    "#display()\n",
    "Markdown(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "Let's try a language classification problem. Using the below starting code, determine the language of: \"Es viernes todavía.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Given a piece of text, classify the langueage it is wrriten in. \\n\n",
    "text: Es viernes todavia? \\n\n",
    "language:\n",
    "\"\"\"\n",
    "print(generation_model.predict(prompt).text)\n",
    "# add code to print the prediction using the defaults for temperature, max output tokens, top_p and k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are a little wordy, use one-shot prompting to get the prediction to return a single word to you, the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following text, classify the language it is written in.\n",
    "\n",
    "Replace this with the one-shot\n",
    "\n",
    "text: Es viernes todaví?\n",
    "language:\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt=prompt,\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction\n",
    "Convert the following list of cooking ingredients to a YAML file with the keys ingredient, quantity, type\n",
    "\n",
    "Ingredients\n",
    "* 9 egg whites\n",
    "* 3/8 tsp Cream of Tartar\n",
    "* 1 1/2 tbs Viniger\n",
    "* 1 1/2 tsp Vanilla\n",
    "* 3 cups Sugar\n",
    "* 1 quarts Heavy whipping cream\n",
    "* 3 boxes Strawberries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "ingredients:\n",
      "  - ingredient: egg whites\n",
      "    quantity: 9\n",
      "    type: eggs\n",
      "  - ingredient: cream of tartar\n",
      "    quantity: 3/8 tsp\n",
      "    type: spice\n",
      "  - ingredient: vinegar\n",
      "    quantity: 1 1/2 tbs\n",
      "    type: liquid\n",
      "  - ingredient: vanilla\n",
      "    quantity: 1 1/2 tsp\n",
      "    type: spice\n",
      "  - ingredient: sugar\n",
      "    quantity: 3 cups\n",
      "    type: sugar\n",
      "  - ingredient: heavy whipping cream\n",
      " \n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Convert the following list of cooking ingredients to a YAML file with the keys ingredient, quantity, type\n",
    "\n",
    "Ingredients\n",
    "* 9 egg whites\n",
    "* 3/8 tsp Cream of Tartar\n",
    "* 1 1/2 tbs Viniger\n",
    "* 1 1/2 tsp Vanilla\n",
    "* 3 cups Sugar\n",
    "* 1 quarts Heavy whipping cream\n",
    "* 3 boxes Strawberries\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt=prompt,\n",
    "    ).text\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work. You have now demonstrated your ability to use many key features in Google's PaLM library. Nice job.We likely want some nice wrapup here, but I don't know what, ha"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_palm_api.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
